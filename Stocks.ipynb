{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c6c24c806d36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tqdm_notebook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd \n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import pickle\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'time' has no attribute 'tzset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1d173375b07c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tensorflow\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TZ'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Asia/Kolkata'\u001b[0m  \u001b[1;31m# to set timezone; needed when running on cloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtzset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'time' has no attribute 'tzset'"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "os.environ['TZ'] = 'Asia/Kolkata'  # to set timezone; needed when running on cloud\n",
    "time.tzset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 2,  # 20<16<10, 25 was a bust\n",
    "    \"epochs\": 300,\n",
    "    \"lr\": 0.00010000,\n",
    "    \"time_steps\": 1\n",
    "}\n",
    "\n",
    "iter_changes = \"dropout_layers_0.4_0.4\"\n",
    "\n",
    "TIME_STEPS = params[\"time_steps\"]\n",
    "BATCH_SIZE = params[\"batch_size\"]\n",
    "stime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time(text, stime):\n",
    "    seconds = (time.time()-stime)\n",
    "    print(text, seconds//60,\"minutes : \",np.round(seconds%60),\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(mat,batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if no_of_rows_drop > 0:\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timeseries(mat, y_col_index):\n",
    "    \"\"\"\n",
    "    Converts ndarray into timeseries format and supervised data format. Takes first TIME_STEPS\n",
    "    number of rows as input and sets the TIME_STEPS+1th data as corresponding output and so on.\n",
    "    :param mat: ndarray which holds the dataset\n",
    "    :param y_col_index: index of column which acts as output\n",
    "    :return: returns two ndarrays-- input and output in format suitable to feed\n",
    "    to LSTM.\n",
    "    \"\"\"\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    print(\"debugs\", dim_0)\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    print(\"dim_0\",dim_0)\n",
    "    for i in tqdm_notebook(range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.decomposition as dec\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import csv\n",
    "stime = time.time()\n",
    "tqdm_notebook.pandas('Processing...')\n",
    "stocks = ['AAPL']\n",
    "daily_dates = ['timestamp', '2019-05-31''2019-05-30''2019-05-29''2019-05-28','2019-05-27','2019-05-26',\n",
    "         '2019-05-25','2019-05-24','2019-05-23','2019-05-22', '2019-05-21', '2019-05-20', '2019-05-19', \n",
    "         '2019-05-18','2019-05-17','2019-05-15','2019-05-15', '2019-05-14', '2019-05-13', '2019-05-12',\n",
    "         '2019-05-11','2019-05-10','2019-05-09','2019-05-08', '2019-05-07', '2019-05-06', '2019-05-05','2019-05-04', \n",
    "         '2019-05-03','2019-05-01','2019-05-01'\n",
    "               \n",
    "         '2019-04-30''2019-04-29''2019-04-28','2019-04-27','2019-04-26',\n",
    "         '2019-04-25','2019-04-24','2019-04-23','2019-04-22', '2019-04-21', '2019-04-20', '2019-04-19', \n",
    "         '2019-04-18','2019-04-17','2019-04-15','2019-04-15', '2019-04-14', '2019-04-13', '2019-04-12',\n",
    "         '2019-04-11','2019-04-10','2019-04-09','2019-04-08', '2019-04-07', '2019-04-06', '2019-04-05','2019-04-04', \n",
    "         '2019-04-03','2019-04-01','2019-04-01' \n",
    "               \n",
    "               '2019-03-29''2019-03-28', '2019-03-27', '2019-03-26',\n",
    "         '2019-03-25','2019-03-24','2019-03-23','2019-03-22', '2019-03-21', '2019-03-20', '2019-03-19', \n",
    "         '2019-03-18','2019-03-17','2019-03-15','2019-03-15', '2019-03-14', '2019-03-13', '2019-03-12',\n",
    "         '2019-03-11','2019-03-10','2019-03-09','2019-03-08', '2019-03-07', '2019-03-06', '2019-03-05','2019-03-04', \n",
    "         '2019-03-03','2019-03-01','2019-03-01'\n",
    "               \n",
    "         '2019-02-28','2019-02-27','2019-02-26',\n",
    "         '2019-02-25','2019-02-24','2019-02-23','2019-02-22', '2019-02-21', '2019-02-20', '2019-02-19', \n",
    "         '2019-02-18','2019-02-17','2019-02-15','2019-02-15', '2019-02-14', '2019-02-13', '2019-02-12',\n",
    "         '2019-02-11','2019-02-10','2019-02-09','2019-02-08', '2019-02-07', '2019-02-06', '2019-02-05','2019-02-04', \n",
    "         '2019-02-03','2019-02-01','2019-02-01'\n",
    "              \n",
    "         '2019-01-31''2019-01-30''2019-01-29''2019-01-28','2019-01-27','2019-01-26',\n",
    "         '2019-01-25','2019-01-24','2019-01-23','2019-01-22', '2019-01-21', '2019-01-20', '2019-01-19', \n",
    "         '2019-01-18','2019-01-17','2019-01-15','2019-01-15', '2019-01-14', '2019-01-13', '2019-01-12',\n",
    "         '2019-01-11','2019-01-10','2019-01-09','2019-01-08', '2019-01-07', '2019-01-06', '2019-01-05','2019-01-04', \n",
    "         '2019-01-03','2019-01-01','2019-01-01' ]\n",
    "\n",
    "for stock in stocks:\n",
    "    ts = TimeSeries(key='TTZNA7B3VCDYYHNZ', output_format='csv')\n",
    "    data, meta_data = ts.get_daily_adjusted(stock, outputsize='full')\n",
    "    with open('{}-daily-prices.csv'.format(stock), mode='w+', newline='') as my_file:\n",
    "        for row in data:\n",
    "            if row[0] in daily_dates:\n",
    "                wr = csv.writer(my_file, quoting=csv.QUOTE_ALL)\n",
    "                wr.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#print(os.listdir(\"../Desktop/finalproject\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"../Desktop/finalproject/AAPL-daily-prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Downloads/AAPL-daily-prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"adjusted_close\", \"dividend_amount\", \"split_coefficient\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-24</td>\n",
       "      <td>180.200</td>\n",
       "      <td>182.1400</td>\n",
       "      <td>178.6200</td>\n",
       "      <td>178.97</td>\n",
       "      <td>23714686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-23</td>\n",
       "      <td>179.800</td>\n",
       "      <td>180.5400</td>\n",
       "      <td>177.8100</td>\n",
       "      <td>179.66</td>\n",
       "      <td>36529736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-22</td>\n",
       "      <td>184.660</td>\n",
       "      <td>185.7100</td>\n",
       "      <td>182.5500</td>\n",
       "      <td>182.78</td>\n",
       "      <td>29748556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>185.220</td>\n",
       "      <td>188.0000</td>\n",
       "      <td>184.7000</td>\n",
       "      <td>186.60</td>\n",
       "      <td>28364848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-20</td>\n",
       "      <td>183.520</td>\n",
       "      <td>184.3490</td>\n",
       "      <td>180.2839</td>\n",
       "      <td>183.09</td>\n",
       "      <td>38612290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-05-17</td>\n",
       "      <td>186.930</td>\n",
       "      <td>190.9000</td>\n",
       "      <td>186.7600</td>\n",
       "      <td>189.00</td>\n",
       "      <td>32879090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-05-15</td>\n",
       "      <td>186.270</td>\n",
       "      <td>191.7500</td>\n",
       "      <td>186.0200</td>\n",
       "      <td>190.92</td>\n",
       "      <td>26544718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>186.410</td>\n",
       "      <td>189.7000</td>\n",
       "      <td>185.4100</td>\n",
       "      <td>188.66</td>\n",
       "      <td>36529677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>187.710</td>\n",
       "      <td>189.4800</td>\n",
       "      <td>182.8500</td>\n",
       "      <td>185.72</td>\n",
       "      <td>57430623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-05-10</td>\n",
       "      <td>197.419</td>\n",
       "      <td>198.8500</td>\n",
       "      <td>192.7700</td>\n",
       "      <td>197.18</td>\n",
       "      <td>41208712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-05-09</td>\n",
       "      <td>200.400</td>\n",
       "      <td>201.6800</td>\n",
       "      <td>196.6600</td>\n",
       "      <td>200.72</td>\n",
       "      <td>34908607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>201.900</td>\n",
       "      <td>205.3400</td>\n",
       "      <td>201.7500</td>\n",
       "      <td>202.90</td>\n",
       "      <td>26339504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-05-07</td>\n",
       "      <td>205.880</td>\n",
       "      <td>207.4175</td>\n",
       "      <td>200.8250</td>\n",
       "      <td>202.86</td>\n",
       "      <td>38763698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>204.290</td>\n",
       "      <td>208.8400</td>\n",
       "      <td>203.5000</td>\n",
       "      <td>208.48</td>\n",
       "      <td>32443113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>210.890</td>\n",
       "      <td>211.8400</td>\n",
       "      <td>210.2300</td>\n",
       "      <td>211.75</td>\n",
       "      <td>20892378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>209.880</td>\n",
       "      <td>215.3100</td>\n",
       "      <td>209.2300</td>\n",
       "      <td>210.52</td>\n",
       "      <td>64827328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-04-26</td>\n",
       "      <td>204.900</td>\n",
       "      <td>205.0000</td>\n",
       "      <td>202.1200</td>\n",
       "      <td>204.30</td>\n",
       "      <td>18649102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-04-25</td>\n",
       "      <td>206.830</td>\n",
       "      <td>207.7600</td>\n",
       "      <td>205.1200</td>\n",
       "      <td>205.28</td>\n",
       "      <td>18543206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-04-24</td>\n",
       "      <td>207.360</td>\n",
       "      <td>208.4800</td>\n",
       "      <td>207.0500</td>\n",
       "      <td>207.16</td>\n",
       "      <td>17540609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-04-23</td>\n",
       "      <td>204.430</td>\n",
       "      <td>207.7500</td>\n",
       "      <td>203.9000</td>\n",
       "      <td>207.48</td>\n",
       "      <td>23322991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-04-22</td>\n",
       "      <td>202.830</td>\n",
       "      <td>204.9400</td>\n",
       "      <td>202.3400</td>\n",
       "      <td>204.53</td>\n",
       "      <td>19439545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-04-18</td>\n",
       "      <td>203.120</td>\n",
       "      <td>204.1500</td>\n",
       "      <td>202.5200</td>\n",
       "      <td>203.86</td>\n",
       "      <td>24195766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019-04-17</td>\n",
       "      <td>199.540</td>\n",
       "      <td>203.3800</td>\n",
       "      <td>198.6100</td>\n",
       "      <td>203.13</td>\n",
       "      <td>28906780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019-04-15</td>\n",
       "      <td>198.580</td>\n",
       "      <td>199.8500</td>\n",
       "      <td>198.0100</td>\n",
       "      <td>199.23</td>\n",
       "      <td>17536646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2019-04-12</td>\n",
       "      <td>199.200</td>\n",
       "      <td>200.1400</td>\n",
       "      <td>196.2100</td>\n",
       "      <td>198.87</td>\n",
       "      <td>27760668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2019-04-11</td>\n",
       "      <td>200.850</td>\n",
       "      <td>201.0000</td>\n",
       "      <td>198.4431</td>\n",
       "      <td>198.95</td>\n",
       "      <td>20900808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>198.680</td>\n",
       "      <td>200.7400</td>\n",
       "      <td>198.1800</td>\n",
       "      <td>200.62</td>\n",
       "      <td>21695288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2019-04-09</td>\n",
       "      <td>200.320</td>\n",
       "      <td>202.8500</td>\n",
       "      <td>199.2300</td>\n",
       "      <td>199.50</td>\n",
       "      <td>35768237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2019-04-08</td>\n",
       "      <td>196.420</td>\n",
       "      <td>200.2300</td>\n",
       "      <td>196.3400</td>\n",
       "      <td>200.10</td>\n",
       "      <td>25881697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>196.450</td>\n",
       "      <td>197.1000</td>\n",
       "      <td>195.9300</td>\n",
       "      <td>197.00</td>\n",
       "      <td>18526644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2019-02-22</td>\n",
       "      <td>171.580</td>\n",
       "      <td>173.0000</td>\n",
       "      <td>171.3800</td>\n",
       "      <td>172.97</td>\n",
       "      <td>18913154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>171.800</td>\n",
       "      <td>172.3700</td>\n",
       "      <td>170.3000</td>\n",
       "      <td>171.06</td>\n",
       "      <td>17249670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2019-02-20</td>\n",
       "      <td>171.190</td>\n",
       "      <td>173.3200</td>\n",
       "      <td>170.9900</td>\n",
       "      <td>172.03</td>\n",
       "      <td>26114362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2019-02-19</td>\n",
       "      <td>169.710</td>\n",
       "      <td>171.4400</td>\n",
       "      <td>169.4900</td>\n",
       "      <td>170.93</td>\n",
       "      <td>18972826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>171.250</td>\n",
       "      <td>171.7000</td>\n",
       "      <td>169.7500</td>\n",
       "      <td>170.42</td>\n",
       "      <td>24626814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>169.710</td>\n",
       "      <td>171.2615</td>\n",
       "      <td>169.3800</td>\n",
       "      <td>170.80</td>\n",
       "      <td>21835747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2019-02-13</td>\n",
       "      <td>171.390</td>\n",
       "      <td>172.4800</td>\n",
       "      <td>169.9200</td>\n",
       "      <td>170.18</td>\n",
       "      <td>22490233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>170.100</td>\n",
       "      <td>171.0000</td>\n",
       "      <td>169.7000</td>\n",
       "      <td>170.89</td>\n",
       "      <td>22283523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2019-02-11</td>\n",
       "      <td>171.050</td>\n",
       "      <td>171.2100</td>\n",
       "      <td>169.2500</td>\n",
       "      <td>169.43</td>\n",
       "      <td>20993425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>168.990</td>\n",
       "      <td>170.6600</td>\n",
       "      <td>168.4200</td>\n",
       "      <td>170.41</td>\n",
       "      <td>23819966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>172.400</td>\n",
       "      <td>173.9400</td>\n",
       "      <td>170.3400</td>\n",
       "      <td>170.94</td>\n",
       "      <td>31741690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>174.650</td>\n",
       "      <td>175.5700</td>\n",
       "      <td>172.8531</td>\n",
       "      <td>174.24</td>\n",
       "      <td>28239591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2019-02-05</td>\n",
       "      <td>172.860</td>\n",
       "      <td>175.0800</td>\n",
       "      <td>172.3501</td>\n",
       "      <td>174.18</td>\n",
       "      <td>36101628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>167.410</td>\n",
       "      <td>171.6550</td>\n",
       "      <td>167.2800</td>\n",
       "      <td>171.25</td>\n",
       "      <td>31495582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>166.960</td>\n",
       "      <td>168.9800</td>\n",
       "      <td>165.9300</td>\n",
       "      <td>166.52</td>\n",
       "      <td>32668138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2019-01-25</td>\n",
       "      <td>155.480</td>\n",
       "      <td>158.1300</td>\n",
       "      <td>154.3200</td>\n",
       "      <td>157.76</td>\n",
       "      <td>33547893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2019-01-24</td>\n",
       "      <td>154.110</td>\n",
       "      <td>154.4800</td>\n",
       "      <td>151.7400</td>\n",
       "      <td>152.70</td>\n",
       "      <td>25441549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2019-01-23</td>\n",
       "      <td>154.150</td>\n",
       "      <td>155.1400</td>\n",
       "      <td>151.7000</td>\n",
       "      <td>153.92</td>\n",
       "      <td>23130570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2019-01-22</td>\n",
       "      <td>156.410</td>\n",
       "      <td>156.7300</td>\n",
       "      <td>152.6200</td>\n",
       "      <td>153.30</td>\n",
       "      <td>30393970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>157.500</td>\n",
       "      <td>157.8800</td>\n",
       "      <td>155.9806</td>\n",
       "      <td>156.82</td>\n",
       "      <td>33751023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>154.200</td>\n",
       "      <td>157.6600</td>\n",
       "      <td>153.2600</td>\n",
       "      <td>155.86</td>\n",
       "      <td>29821160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>150.270</td>\n",
       "      <td>153.3900</td>\n",
       "      <td>150.0500</td>\n",
       "      <td>153.07</td>\n",
       "      <td>28710324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>150.850</td>\n",
       "      <td>151.2700</td>\n",
       "      <td>149.2200</td>\n",
       "      <td>150.00</td>\n",
       "      <td>32439186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2019-01-11</td>\n",
       "      <td>152.880</td>\n",
       "      <td>153.7000</td>\n",
       "      <td>151.5100</td>\n",
       "      <td>152.29</td>\n",
       "      <td>27023241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>152.500</td>\n",
       "      <td>153.9700</td>\n",
       "      <td>150.8600</td>\n",
       "      <td>153.80</td>\n",
       "      <td>35780670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>151.290</td>\n",
       "      <td>154.5300</td>\n",
       "      <td>149.6300</td>\n",
       "      <td>153.31</td>\n",
       "      <td>45099081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>149.560</td>\n",
       "      <td>151.8200</td>\n",
       "      <td>148.5200</td>\n",
       "      <td>150.75</td>\n",
       "      <td>41025314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>148.700</td>\n",
       "      <td>148.8300</td>\n",
       "      <td>145.9000</td>\n",
       "      <td>147.93</td>\n",
       "      <td>54777764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>144.530</td>\n",
       "      <td>148.5499</td>\n",
       "      <td>143.8000</td>\n",
       "      <td>148.26</td>\n",
       "      <td>58607070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>143.980</td>\n",
       "      <td>145.7200</td>\n",
       "      <td>142.0000</td>\n",
       "      <td>142.19</td>\n",
       "      <td>91312195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamp     open      high       low   close    volume\n",
       "0   2019-05-24  180.200  182.1400  178.6200  178.97  23714686\n",
       "1   2019-05-23  179.800  180.5400  177.8100  179.66  36529736\n",
       "2   2019-05-22  184.660  185.7100  182.5500  182.78  29748556\n",
       "3   2019-05-21  185.220  188.0000  184.7000  186.60  28364848\n",
       "4   2019-05-20  183.520  184.3490  180.2839  183.09  38612290\n",
       "5   2019-05-17  186.930  190.9000  186.7600  189.00  32879090\n",
       "6   2019-05-15  186.270  191.7500  186.0200  190.92  26544718\n",
       "7   2019-05-14  186.410  189.7000  185.4100  188.66  36529677\n",
       "8   2019-05-13  187.710  189.4800  182.8500  185.72  57430623\n",
       "9   2019-05-10  197.419  198.8500  192.7700  197.18  41208712\n",
       "10  2019-05-09  200.400  201.6800  196.6600  200.72  34908607\n",
       "11  2019-05-08  201.900  205.3400  201.7500  202.90  26339504\n",
       "12  2019-05-07  205.880  207.4175  200.8250  202.86  38763698\n",
       "13  2019-05-06  204.290  208.8400  203.5000  208.48  32443113\n",
       "14  2019-05-03  210.890  211.8400  210.2300  211.75  20892378\n",
       "15  2019-05-01  209.880  215.3100  209.2300  210.52  64827328\n",
       "16  2019-04-26  204.900  205.0000  202.1200  204.30  18649102\n",
       "17  2019-04-25  206.830  207.7600  205.1200  205.28  18543206\n",
       "18  2019-04-24  207.360  208.4800  207.0500  207.16  17540609\n",
       "19  2019-04-23  204.430  207.7500  203.9000  207.48  23322991\n",
       "20  2019-04-22  202.830  204.9400  202.3400  204.53  19439545\n",
       "21  2019-04-18  203.120  204.1500  202.5200  203.86  24195766\n",
       "22  2019-04-17  199.540  203.3800  198.6100  203.13  28906780\n",
       "23  2019-04-15  198.580  199.8500  198.0100  199.23  17536646\n",
       "24  2019-04-12  199.200  200.1400  196.2100  198.87  27760668\n",
       "25  2019-04-11  200.850  201.0000  198.4431  198.95  20900808\n",
       "26  2019-04-10  198.680  200.7400  198.1800  200.62  21695288\n",
       "27  2019-04-09  200.320  202.8500  199.2300  199.50  35768237\n",
       "28  2019-04-08  196.420  200.2300  196.3400  200.10  25881697\n",
       "29  2019-04-05  196.450  197.1000  195.9300  197.00  18526644\n",
       "..         ...      ...       ...       ...     ...       ...\n",
       "55  2019-02-22  171.580  173.0000  171.3800  172.97  18913154\n",
       "56  2019-02-21  171.800  172.3700  170.3000  171.06  17249670\n",
       "57  2019-02-20  171.190  173.3200  170.9900  172.03  26114362\n",
       "58  2019-02-19  169.710  171.4400  169.4900  170.93  18972826\n",
       "59  2019-02-15  171.250  171.7000  169.7500  170.42  24626814\n",
       "60  2019-02-14  169.710  171.2615  169.3800  170.80  21835747\n",
       "61  2019-02-13  171.390  172.4800  169.9200  170.18  22490233\n",
       "62  2019-02-12  170.100  171.0000  169.7000  170.89  22283523\n",
       "63  2019-02-11  171.050  171.2100  169.2500  169.43  20993425\n",
       "64  2019-02-08  168.990  170.6600  168.4200  170.41  23819966\n",
       "65  2019-02-07  172.400  173.9400  170.3400  170.94  31741690\n",
       "66  2019-02-06  174.650  175.5700  172.8531  174.24  28239591\n",
       "67  2019-02-05  172.860  175.0800  172.3501  174.18  36101628\n",
       "68  2019-02-04  167.410  171.6550  167.2800  171.25  31495582\n",
       "69  2019-02-01  166.960  168.9800  165.9300  166.52  32668138\n",
       "70  2019-01-25  155.480  158.1300  154.3200  157.76  33547893\n",
       "71  2019-01-24  154.110  154.4800  151.7400  152.70  25441549\n",
       "72  2019-01-23  154.150  155.1400  151.7000  153.92  23130570\n",
       "73  2019-01-22  156.410  156.7300  152.6200  153.30  30393970\n",
       "74  2019-01-18  157.500  157.8800  155.9806  156.82  33751023\n",
       "75  2019-01-17  154.200  157.6600  153.2600  155.86  29821160\n",
       "76  2019-01-15  150.270  153.3900  150.0500  153.07  28710324\n",
       "77  2019-01-14  150.850  151.2700  149.2200  150.00  32439186\n",
       "78  2019-01-11  152.880  153.7000  151.5100  152.29  27023241\n",
       "79  2019-01-10  152.500  153.9700  150.8600  153.80  35780670\n",
       "80  2019-01-09  151.290  154.5300  149.6300  153.31  45099081\n",
       "81  2019-01-08  149.560  151.8200  148.5200  150.75  41025314\n",
       "82  2019-01-07  148.700  148.8300  145.9000  147.93  54777764\n",
       "83  2019-01-04  144.530  148.5499  143.8000  148.26  58607070\n",
       "84  2019-01-03  143.980  145.7200  142.0000  142.19  91312195\n",
       "\n",
       "[85 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test size 68 17\n"
     ]
    }
   ],
   "source": [
    "train_cols = [\"open\",\"high\",\"low\",\"close\",\"volume\"]\n",
    "df_train, df_test = train_test_split(df, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "print(\"Train and Test size\", len(df_train), len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the feature MinMax, build array\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting unused dataframes of total size(KB) 18\n"
     ]
    }
   ],
   "source": [
    "print(\"Deleting unused dataframes of total size(KB)\",(sys.getsizeof(df)+sys.getsizeof(df_train)+sys.getsizeof(df_test))//1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking if any null values are present\n",
      " timestamp    0\n",
      "open         0\n",
      "high         0\n",
      "low          0\n",
      "close        0\n",
      "volume       0\n",
      "dtype: int64\n",
      "debugs 67\n",
      "dim_0 67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5b508c198a469888d384b46757238c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=67), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (67, 1, 5) (67,)\n",
      "Batch trimmed size (66, 1, 5) (66,)\n"
     ]
    }
   ],
   "source": [
    "print(\"checking if any null values are present\\n\", df.isna().sum())\n",
    "x_t, y_t = build_timeseries(x_train,3)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "print(\"Batch trimmed size\",x_t.shape, y_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    lstm_model = Sequential()\n",
    "    # (batch_size, timesteps, data_dim)\n",
    "    lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]),\n",
    "                        dropout=0.0, recurrent_dropout=0.0, stateful=True, return_sequences=True,\n",
    "                        kernel_initializer='random_uniform'))\n",
    "    lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(LSTM(60, dropout=0.0))\n",
    "    lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(Dense(20,activation='relu'))\n",
    "    lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "    optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    # optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved model...\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "try:\n",
    "    model = pickle.load(open(\"lstm_model\", 'rb'))\n",
    "    print(\"Loaded saved model...\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Model not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debugs 16\n",
      "dim_0 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d79f349999a47ca9a96b1b4e6e72c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (16, 1, 5) (16,)\n",
      "Test size (8, 1, 5) (8,) (8, 1, 5) (8,)\n",
      "[[[-0.03770883  0.02228443 -0.0272662   0.04300567  0.30205699]]\n",
      "\n",
      " [[-0.04844869 -0.03762598 -0.05955513 -0.06876181  0.32660948]]\n",
      "\n",
      " [[-0.32243437 -0.2806271  -0.33723989 -0.27575614  0.34503092]]\n",
      "\n",
      " [[-0.35513126 -0.36237402 -0.39894762 -0.39532136  0.17528985]]\n",
      "\n",
      " [[-0.35417661 -0.34759239 -0.39990433 -0.36649338  0.1268996 ]]\n",
      "\n",
      " [[-0.30023866 -0.31198208 -0.37790002 -0.38114367  0.27899002]]\n",
      "\n",
      " [[-0.27422434 -0.2862262  -0.29752212 -0.29796786  0.34928432]]\n",
      "\n",
      " [[-0.35298329 -0.29115342 -0.36259268 -0.32065217  0.26699579]]\n",
      "\n",
      " [[-0.44677804 -0.38678611 -0.43936857 -0.38657845  0.24373567]]\n",
      "\n",
      " [[-0.43293556 -0.43426652 -0.45922028 -0.45912098  0.32181539]]\n",
      "\n",
      " [[-0.38448687 -0.37984323 -0.4044487  -0.40500945  0.20840936]]\n",
      "\n",
      " [[-0.39355609 -0.37379619 -0.41999522 -0.36932892  0.39178368]]\n",
      "\n",
      " [[-0.42243437 -0.3612542  -0.44941402 -0.38090737  0.58690457]]\n",
      "\n",
      " [[-0.46372315 -0.42194849 -0.47596269 -0.44139887  0.50160279]]\n",
      "\n",
      " [[-0.48424821 -0.48891377 -0.53862712 -0.50803403  0.78956929]]\n",
      "\n",
      " [[-0.58377088 -0.49518701 -0.58885434 -0.50023629  0.86975223]]]\n"
     ]
    }
   ],
   "source": [
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n",
    "\n",
    "print(\"Test size\", x_test_t.shape, y_test_t.shape, x_val.shape, y_val.shape)\n",
    "print(x_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH=\"C:\\\\Users\\\\robsm\\\\Downloads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "checking if GPU available []\n",
      "Train on 66 samples, validate on 8 samples\n",
      "Epoch 1/300\n",
      " - 2s - loss: 0.0957 - val_loss: 0.6664\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66635, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 2/300\n",
      " - 0s - loss: 0.0954 - val_loss: 0.6639\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66635 to 0.66392, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 3/300\n",
      " - 0s - loss: 0.0952 - val_loss: 0.6614\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.66392 to 0.66139, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 4/300\n",
      " - 0s - loss: 0.0948 - val_loss: 0.6587\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.66139 to 0.65871, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 5/300\n",
      " - 0s - loss: 0.0944 - val_loss: 0.6557\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.65871 to 0.65570, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 6/300\n",
      " - 0s - loss: 0.0940 - val_loss: 0.6527\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.65570 to 0.65274, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 7/300\n",
      " - 0s - loss: 0.0936 - val_loss: 0.6494\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.65274 to 0.64940, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 8/300\n",
      " - 0s - loss: 0.0932 - val_loss: 0.6460\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.64940 to 0.64596, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 9/300\n",
      " - 0s - loss: 0.0928 - val_loss: 0.6424\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.64596 to 0.64238, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 10/300\n",
      " - 0s - loss: 0.0926 - val_loss: 0.6387\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.64238 to 0.63874, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 11/300\n",
      " - 0s - loss: 0.0922 - val_loss: 0.6348\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.63874 to 0.63476, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 12/300\n",
      " - 0s - loss: 0.0921 - val_loss: 0.6309\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.63476 to 0.63090, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 13/300\n",
      " - 0s - loss: 0.0913 - val_loss: 0.6267\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.63090 to 0.62666, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 14/300\n",
      " - 0s - loss: 0.0913 - val_loss: 0.6224\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.62666 to 0.62243, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 15/300\n",
      " - 0s - loss: 0.0909 - val_loss: 0.6177\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.62243 to 0.61771, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 16/300\n",
      " - 0s - loss: 0.0906 - val_loss: 0.6131\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.61771 to 0.61312, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 17/300\n",
      " - 0s - loss: 0.0900 - val_loss: 0.6082\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.61312 to 0.60820, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 18/300\n",
      " - 0s - loss: 0.0894 - val_loss: 0.6029\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.60820 to 0.60291, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 19/300\n",
      " - 0s - loss: 0.0888 - val_loss: 0.5970\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.60291 to 0.59704, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 20/300\n",
      " - 0s - loss: 0.0887 - val_loss: 0.5915\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.59704 to 0.59149, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 21/300\n",
      " - 0s - loss: 0.0885 - val_loss: 0.5863\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.59149 to 0.58629, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 22/300\n",
      " - 0s - loss: 0.0875 - val_loss: 0.5801\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.58629 to 0.58013, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 23/300\n",
      " - 0s - loss: 0.0875 - val_loss: 0.5736\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.58013 to 0.57363, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 24/300\n",
      " - 0s - loss: 0.0866 - val_loss: 0.5667\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.57363 to 0.56669, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 25/300\n",
      " - 0s - loss: 0.0853 - val_loss: 0.5588\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.56669 to 0.55875, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 26/300\n",
      " - 0s - loss: 0.0843 - val_loss: 0.5508\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.55875 to 0.55079, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 27/300\n",
      " - 0s - loss: 0.0840 - val_loss: 0.5417\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.55079 to 0.54166, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 28/300\n",
      " - 0s - loss: 0.0830 - val_loss: 0.5307\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.54166 to 0.53073, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 29/300\n",
      " - 0s - loss: 0.0812 - val_loss: 0.5196\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.53073 to 0.51958, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 30/300\n",
      " - 0s - loss: 0.0809 - val_loss: 0.5072\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.51958 to 0.50723, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 31/300\n",
      " - 0s - loss: 0.0775 - val_loss: 0.4921\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.50723 to 0.49213, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 32/300\n",
      " - 0s - loss: 0.0769 - val_loss: 0.4764\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.49213 to 0.47637, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 33/300\n",
      " - 0s - loss: 0.0747 - val_loss: 0.4586\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.47637 to 0.45857, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 34/300\n",
      " - 0s - loss: 0.0724 - val_loss: 0.4376\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.45857 to 0.43763, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 35/300\n",
      " - 0s - loss: 0.0683 - val_loss: 0.4118\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.43763 to 0.41181, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 36/300\n",
      " - 0s - loss: 0.0642 - val_loss: 0.3813\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.41181 to 0.38134, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 37/300\n",
      " - 0s - loss: 0.0558 - val_loss: 0.3488\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.38134 to 0.34881, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 38/300\n",
      " - 0s - loss: 0.0510 - val_loss: 0.3091\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.34881 to 0.30909, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 39/300\n",
      " - 0s - loss: 0.0474 - val_loss: 0.2737\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.30909 to 0.27373, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 40/300\n",
      " - 0s - loss: 0.0448 - val_loss: 0.2436\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.27373 to 0.24362, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 41/300\n",
      " - 0s - loss: 0.0380 - val_loss: 0.2240\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.24362 to 0.22403, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 42/300\n",
      " - 0s - loss: 0.0325 - val_loss: 0.2113\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.22403 to 0.21131, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 43/300\n",
      " - 0s - loss: 0.0240 - val_loss: 0.1970\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.21131 to 0.19696, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 44/300\n",
      " - 0s - loss: 0.0202 - val_loss: 0.1888\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.19696 to 0.18882, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 45/300\n",
      " - 0s - loss: 0.0186 - val_loss: 0.1922\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.18882\n",
      "Epoch 46/300\n",
      " - 0s - loss: 0.0126 - val_loss: 0.1918\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.18882\n",
      "Epoch 47/300\n",
      " - 0s - loss: 0.0143 - val_loss: 0.1905\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.18882\n",
      "Epoch 48/300\n",
      " - 0s - loss: 0.0151 - val_loss: 0.1939\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.18882\n",
      "Epoch 49/300\n",
      " - 0s - loss: 0.0121 - val_loss: 0.1985\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.18882\n",
      "Epoch 50/300\n",
      " - 0s - loss: 0.0113 - val_loss: 0.2078\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.18882\n",
      "Epoch 51/300\n",
      " - 0s - loss: 0.0131 - val_loss: 0.2100\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.18882\n",
      "Epoch 52/300\n",
      " - 0s - loss: 0.0104 - val_loss: 0.2092\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.18882\n",
      "Epoch 53/300\n",
      " - 0s - loss: 0.0097 - val_loss: 0.2137\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.18882\n",
      "Epoch 54/300\n",
      " - 0s - loss: 0.0094 - val_loss: 0.2062\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.18882\n",
      "Epoch 55/300\n",
      " - 0s - loss: 0.0121 - val_loss: 0.2101\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.18882\n",
      "Epoch 56/300\n",
      " - 0s - loss: 0.0104 - val_loss: 0.2141\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.18882\n",
      "Epoch 57/300\n",
      " - 0s - loss: 0.0114 - val_loss: 0.2157\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.18882\n",
      "Epoch 58/300\n",
      " - 0s - loss: 0.0106 - val_loss: 0.1960\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.18882\n",
      "Epoch 59/300\n",
      " - 0s - loss: 0.0094 - val_loss: 0.1907\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.18882\n",
      "Epoch 60/300\n",
      " - 0s - loss: 0.0119 - val_loss: 0.1997\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.18882\n",
      "Epoch 61/300\n",
      " - 0s - loss: 0.0098 - val_loss: 0.2287\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.18882\n",
      "Epoch 62/300\n",
      " - 0s - loss: 0.0137 - val_loss: 0.2459\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.18882\n",
      "Epoch 63/300\n",
      " - 0s - loss: 0.0101 - val_loss: 0.2039\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.18882\n",
      "Epoch 64/300\n",
      " - 0s - loss: 0.0123 - val_loss: 0.1751\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.18882 to 0.17511, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 65/300\n",
      " - 0s - loss: 0.0153 - val_loss: 0.1735\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.17511 to 0.17352, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 66/300\n",
      " - 0s - loss: 0.0111 - val_loss: 0.1882\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.17352\n",
      "Epoch 67/300\n",
      " - 0s - loss: 0.0107 - val_loss: 0.2360\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.17352\n",
      "Epoch 68/300\n",
      " - 0s - loss: 0.0140 - val_loss: 0.2615\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.17352\n",
      "Epoch 69/300\n",
      " - 0s - loss: 0.0139 - val_loss: 0.2112\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.17352\n",
      "Epoch 70/300\n",
      " - 0s - loss: 0.0088 - val_loss: 0.1628\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.17352 to 0.16276, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 71/300\n",
      " - 0s - loss: 0.0263 - val_loss: 0.1601\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.16276 to 0.16015, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 72/300\n",
      " - 0s - loss: 0.0215 - val_loss: 0.1722\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.16015\n",
      "Epoch 73/300\n",
      " - 0s - loss: 0.0084 - val_loss: 0.2063\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.16015\n",
      "Epoch 74/300\n",
      " - 0s - loss: 0.0115 - val_loss: 0.2467\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.16015\n",
      "Epoch 75/300\n",
      " - 0s - loss: 0.0132 - val_loss: 0.2217\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.16015\n",
      "Epoch 76/300\n",
      " - 0s - loss: 0.0083 - val_loss: 0.1719\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.16015\n",
      "Epoch 77/300\n",
      " - 0s - loss: 0.0185 - val_loss: 0.1557\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.16015 to 0.15573, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 78/300\n",
      " - 0s - loss: 0.0223 - val_loss: 0.1613\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.15573\n",
      "Epoch 79/300\n",
      " - 0s - loss: 0.0102 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.15573\n",
      "Epoch 80/300\n",
      " - 0s - loss: 0.0093 - val_loss: 0.2129\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.15573\n",
      "Epoch 81/300\n",
      " - 0s - loss: 0.0105 - val_loss: 0.2217\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.15573\n",
      "Epoch 82/300\n",
      " - 0s - loss: 0.0131 - val_loss: 0.1752\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.15573\n",
      "Epoch 83/300\n",
      " - 0s - loss: 0.0150 - val_loss: 0.1458\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.15573 to 0.14577, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 84/300\n",
      " - 0s - loss: 0.0386 - val_loss: 0.1455\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.14577 to 0.14547, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 85/300\n",
      " - 0s - loss: 0.0274 - val_loss: 0.1559\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.14547\n",
      "Epoch 86/300\n",
      " - 0s - loss: 0.0091 - val_loss: 0.2011\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.14547\n",
      "Epoch 87/300\n",
      " - 0s - loss: 0.0120 - val_loss: 0.3301\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.14547\n",
      "Epoch 88/300\n",
      " - 0s - loss: 0.0287 - val_loss: 0.5215\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.14547\n",
      "Epoch 89/300\n",
      " - 0s - loss: 0.0378 - val_loss: 0.6040\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.14547\n",
      "Epoch 90/300\n",
      " - 0s - loss: 0.0338 - val_loss: 0.5107\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.14547\n",
      "Epoch 91/300\n",
      " - 0s - loss: 0.0244 - val_loss: 0.2622\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.14547\n",
      "Epoch 92/300\n",
      " - 0s - loss: 0.0257 - val_loss: 0.1541\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.14547\n",
      "Epoch 93/300\n",
      " - 0s - loss: 0.0886 - val_loss: 0.1505\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.14547\n",
      "Epoch 94/300\n",
      " - 0s - loss: 0.0866 - val_loss: 0.1469\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.14547\n",
      "Epoch 95/300\n",
      " - 0s - loss: 0.0419 - val_loss: 0.1475\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.14547\n",
      "Epoch 96/300\n",
      " - 0s - loss: 0.0168 - val_loss: 0.1498\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.14547\n",
      "Epoch 97/300\n",
      " - 0s - loss: 0.0123 - val_loss: 0.1578\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.14547\n",
      "Epoch 98/300\n",
      " - 0s - loss: 0.0091 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.14547\n",
      "Epoch 99/300\n",
      " - 0s - loss: 0.0096 - val_loss: 0.2009\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.14547\n",
      "Epoch 100/300\n",
      " - 0s - loss: 0.0109 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.14547\n",
      "Epoch 101/300\n",
      " - 0s - loss: 0.0087 - val_loss: 0.1443\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.14547 to 0.14425, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 102/300\n",
      " - 0s - loss: 0.0221 - val_loss: 0.1380\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.14425 to 0.13803, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 103/300\n",
      " - 0s - loss: 0.0220 - val_loss: 0.1427\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.13803\n",
      "Epoch 104/300\n",
      " - 0s - loss: 0.0109 - val_loss: 0.1702\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.13803\n",
      "Epoch 105/300\n",
      " - 0s - loss: 0.0118 - val_loss: 0.3344\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.13803\n",
      "Epoch 106/300\n",
      " - 0s - loss: 0.0317 - val_loss: 0.7577\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.13803\n",
      "Epoch 107/300\n",
      " - 0s - loss: 0.0820 - val_loss: 0.8785\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.13803\n",
      "Epoch 108/300\n",
      " - 0s - loss: 0.0959 - val_loss: 0.5569\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.13803\n",
      "Epoch 109/300\n",
      " - 0s - loss: 0.0289 - val_loss: 0.4693\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.13803\n",
      "Epoch 110/300\n",
      " - 0s - loss: 0.0224 - val_loss: 0.1915\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.13803\n",
      "Epoch 111/300\n",
      " - 0s - loss: 0.0243 - val_loss: 0.1302\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.13803 to 0.13021, saving model to C:\\Users\\robsm\\Downloads\\best_model.h5\n",
      "Epoch 112/300\n",
      " - 0s - loss: 0.1891 - val_loss: 0.1492\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.13021\n",
      "Epoch 113/300\n",
      " - 0s - loss: 0.0987 - val_loss: 0.1357\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.13021\n",
      "Epoch 114/300\n",
      " - 0s - loss: 0.0493 - val_loss: 0.1389\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.13021\n",
      "Epoch 115/300\n",
      " - 0s - loss: 0.0177 - val_loss: 0.1481\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.13021\n",
      "Epoch 116/300\n",
      " - 0s - loss: 0.0112 - val_loss: 0.1654\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.13021\n",
      "Epoch 117/300\n",
      " - 0s - loss: 0.0095 - val_loss: 0.1652\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.13021\n",
      "Epoch 118/300\n",
      " - 0s - loss: 0.0089 - val_loss: 0.1467\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.13021\n",
      "Epoch 119/300\n",
      " - 0s - loss: 0.0116 - val_loss: 0.1397\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.13021\n",
      "Epoch 120/300\n",
      " - 0s - loss: 0.0150 - val_loss: 0.1407\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.13021\n",
      "Epoch 121/300\n",
      " - 0s - loss: 0.0121 - val_loss: 0.1578\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.13021\n",
      "Epoch 122/300\n",
      " - 0s - loss: 0.0111 - val_loss: 0.2550\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.13021\n",
      "Epoch 123/300\n",
      " - 0s - loss: 0.0229 - val_loss: 0.5319\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.13021\n",
      "Epoch 124/300\n",
      " - 0s - loss: 0.0492 - val_loss: 0.8298\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.13021\n",
      "Epoch 125/300\n",
      " - 0s - loss: 0.0956 - val_loss: 0.6972\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.13021\n",
      "Epoch 126/300\n",
      " - 0s - loss: 0.0601 - val_loss: 0.5294\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.13021\n",
      "Epoch 127/300\n",
      " - 0s - loss: 0.0269 - val_loss: 0.3410\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.13021\n",
      "Epoch 128/300\n",
      " - 0s - loss: 0.0145 - val_loss: 0.1433\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.13021\n",
      "Epoch 129/300\n",
      " - 0s - loss: 0.0659 - val_loss: 0.1325\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.13021\n",
      "Epoch 130/300\n",
      " - 0s - loss: 0.1052 - val_loss: 0.1340\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.13021\n",
      "Epoch 131/300\n",
      " - 0s - loss: 0.0648 - val_loss: 0.1381\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.13021\n",
      "Epoch 132/300\n",
      " - 0s - loss: 0.0272 - val_loss: 0.1469\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.13021\n",
      "Epoch 133/300\n",
      " - 0s - loss: 0.0090 - val_loss: 0.1563\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.13021\n",
      "Epoch 134/300\n",
      " - 0s - loss: 0.0073 - val_loss: 0.1695\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.13021\n",
      "Epoch 135/300\n",
      " - 0s - loss: 0.0082 - val_loss: 0.1752\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.13021\n",
      "Epoch 136/300\n",
      " - 0s - loss: 0.0093 - val_loss: 0.1615\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.13021\n",
      "Epoch 137/300\n",
      " - 0s - loss: 0.0084 - val_loss: 0.1482\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.13021\n",
      "Epoch 138/300\n",
      " - 0s - loss: 0.0144 - val_loss: 0.1488\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.13021\n",
      "Epoch 139/300\n",
      " - 0s - loss: 0.0098 - val_loss: 0.1740\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.13021\n",
      "Epoch 140/300\n",
      " - 0s - loss: 0.0094 - val_loss: 0.3063\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.13021\n",
      "Epoch 141/300\n",
      " - 0s - loss: 0.0252 - val_loss: 0.6260\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.13021\n",
      "Epoch 142/300\n",
      " - 0s - loss: 0.0552 - val_loss: 0.8081\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.13021\n",
      "Epoch 143/300\n",
      " - 0s - loss: 0.0696 - val_loss: 0.6579\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.13021\n",
      "Epoch 144/300\n",
      " - 0s - loss: 0.0287 - val_loss: 0.4545\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.13021\n",
      "Epoch 145/300\n",
      " - 0s - loss: 0.0174 - val_loss: 0.1583\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.13021\n",
      "Epoch 146/300\n",
      " - 0s - loss: 0.0464 - val_loss: 0.1327\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.13021\n",
      "Epoch 147/300\n",
      " - 0s - loss: 0.0835 - val_loss: 0.1337\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.13021\n",
      "Epoch 148/300\n",
      " - 0s - loss: 0.0557 - val_loss: 0.1382\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.13021\n",
      "Epoch 149/300\n",
      " - 0s - loss: 0.0285 - val_loss: 0.1487\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.13021\n",
      "Epoch 150/300\n",
      " - 0s - loss: 0.0103 - val_loss: 0.1715\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.13021\n",
      "Epoch 151/300\n",
      " - 0s - loss: 0.0089 - val_loss: 0.2189\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.13021\n",
      "Epoch 00151: early stopping\n",
      "saving model...\n"
     ]
    }
   ],
   "source": [
    "is_update_model = True\n",
    "if model is None or is_update_model:\n",
    "    from keras import backend as K\n",
    "    print(\"Building model...\")\n",
    "    print(\"checking if GPU available\", K.tensorflow_backend._get_available_gpus())\n",
    "    model = create_model()\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=40, min_delta=0.0001)\n",
    "    \n",
    "    mcp = ModelCheckpoint(os.path.join(OUTPUT_PATH,\n",
    "                          \"best_model.h5\"), monitor='val_loss', verbose=1,\n",
    "                          save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "\n",
    "    # Not used here. But leaving it here as a reminder for future\n",
    "    r_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, \n",
    "                                  verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    \n",
    "    csv_logger = CSVLogger('training.log')\n",
    "    \n",
    "    history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=2, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                        trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, mcp, csv_logger])\n",
    "    \n",
    "    print(\"saving model...\")\n",
    "    pickle.dump(model, open(\"lstm_model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error is 0.8020460430778629 (8,) (8,)\n",
      "[0.27486265 0.25210738 0.38282984 0.35593966 0.49367544 0.46094504\n",
      " 0.56103826 0.5385169 ]\n",
      "[-0.45912098 -0.40500945 -0.36932892 -0.38090737 -0.44139887 -0.50803403\n",
      " -0.50023629 -0.6436673 ]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "y_pred = model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "print(y_pred[0:15])\n",
    "print(y_test_t[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181.06218 180.09918 185.63135 184.49336 190.32234 188.9372  193.17313\n",
      " 192.22003]\n",
      "[150.   152.29 153.8  153.31 150.75 147.93 148.26 142.19]\n"
     ]
    }
   ],
   "source": [
    "# convert the predicted value to range of real data\n",
    "y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# min_max_scaler.inverse_transform(y_pred)\n",
    "y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# min_max_scaler.inverse_transform(y_test_t)\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the training data\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "#plt.show()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'train_vis_BS_.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x000001D34E62CB00>\n"
     ]
    }
   ],
   "source": [
    "# load the saved best model from above\n",
    "saved_model = load_model(os.path.join(OUTPUT_PATH, 'best_model.h5')) # , \"lstm_best_7-3-19_12AM\",\n",
    "print(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error is 0.6438510064176715 (8,) (8,)\n",
      "[0.40923783 0.40643197 0.3638909  0.3608959  0.31752944 0.31151032\n",
      " 0.266797   0.25881553]\n",
      "[-0.45912098 -0.40500945 -0.36932892 -0.38090737 -0.44139887 -0.50803403\n",
      " -0.50023629 -0.6436673 ]\n",
      "[186.74893 186.63019 184.82985 184.70311 182.86784 182.61311 180.72084\n",
      " 180.38307]\n",
      "[150.   152.29 153.8  153.31 150.75 147.93 148.26 142.19]\n"
     ]
    }
   ],
   "source": [
    "y_pred = saved_model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "print(y_pred[0:15])\n",
    "print(y_test_t[0:15])\n",
    "y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_pred)\n",
    "y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program completed  3.0 minutes :  39.0 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the prediction\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(y_pred_org)\n",
    "plt.plot(y_test_t_org)\n",
    "plt.title('Prediction vs Real Stock Price')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "#plt.show()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'pred_vs_real_BS.png'))\n",
    "print_time(\"program completed \", stime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
